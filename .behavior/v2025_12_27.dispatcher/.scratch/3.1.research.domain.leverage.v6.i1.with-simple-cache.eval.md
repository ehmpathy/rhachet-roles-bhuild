# leverage evaluation: with-simple-cache

**source:** https://github.com/ehmpathy/with-simple-cache


---


## 1. tool description

with-simple-cache is a lightweight wrapper that adds caching to any function with minimal code changes. it:
- automatically generates cache keys from function arguments
- supports sync and async functions
- works with any cache backend (in-memory, disk, S3, redis, etc)
- handles serialization of keys and values
- preserves function signatures via wrapper pattern


---


## 2. leverage narrative

### 2.1. what problem does it solve?

implementing caching by hand is:
- **repetitive**: same pattern (check cache → compute → store) repeated everywhere
- **error-prone**: cache key collisions, serialization bugs, stale data issues
- **invasive**: cache logic pollutes business logic
- **inconsistent**: different implementations across codebase

### 2.2. how does it create leverage?

the wrapper moves caching complexity from **scattered implementations** to **single wrapper**:

```
wout with-simple-cache:
  per function: 15-30 lines of cache boilerplate
  risk: key collisions, serialization bugs, logic pollution

with with-simple-cache:
  per function: 1 line wrapper
  cache logic: 0 lines in business code
```

**leverage ratio:** ~20-30x reduction in cache-related code

### 2.3. where does leverage compound?

1. **simpler → safer**: less code = fewer bugs
2. **reusable → authorable**: same wrapper works for any function
3. **pitofsuccess → supportable**: automatic key generation eliminates collision bugs


---


## 3. leverage measurements

### 3.1. leverage.authorable

| subdimension | score | rationale |
|--------------|-------|-----------|
| **simpler** | 10/10 | one-line wrapper; zero cache logic in business code |
| **faster** | 9/10 | seconds to add caching vs minutes of boilerplate |
| **safer** | 9/10 | automatic key generation eliminates collision bugs |

**composite:** 9.3/10

**narrative:** "wrap any function in one line to add caching. zero cognitive load on cache implementation details."


### 3.2. leverage.supportable

| subdimension | score | rationale |
|--------------|-------|-----------|
| **observable** | 6/10 | cache hits/misses not logged by default |
| **intuitive** | 9/10 | wrapper pattern is predictable; behavior is clear |
| **resilient** | 7/10 | cache failures handled, but no circuit breaker |
| **maintainable** | 9/10 | cache logic centralized; change once, apply everywhere |

**composite:** 7.8/10

**narrative:** "the wrapper pattern makes caching behavior predictable. cache issues are isolated from business logic."


### 3.3. leverage.adoptable

| subdimension | score | rationale |
|--------------|-------|-----------|
| **reusable** | 10/10 | works with any function, any cache backend |
| **pitofsuccess** | 9/10 | automatic key generation prevents collisions; type-safe |
| **documented** | 7/10 | README covers basics; advanced patterns need exploration |
| **evolvable** | 8/10 | swap cache backends without changing wrapped functions |

**composite:** 8.5/10

**narrative:** "the wrapper works with any function and any cache backend. switching from in-memory to redis requires zero changes to wrapped code."


---


## 4. gain analysis

### 4.1. absolute gain per use

| metric | wout wrapper | with wrapper | gain |
|--------|--------------|--------------|------|
| lines per cached function | 15-30 | 1 | 15-30x |
| cache key bugs | common | rare | ~10x safer |
| time to add caching | 10-20 min | 30 sec | 20-40x |
| backend switch effort | refactor all | change 1 line | ∞ |

**absolute gain per use:** ~25x authorship velocity, ~10x safety


### 4.2. relative frequency (how often we gain)

| scenario | frequency | notes |
|----------|-----------|-------|
| functions needing caching | medium (10-20%) | expensive computations, API calls |
| cache backend changes | low (rare) | but high impact when needed |
| new projects with caching needs | high | most services need some caching |

**coverage:** ~15% of functions benefit directly; 100% of projects with caching needs


### 4.3. compound leverage

```
total_leverage = absolute_gain × frequency × compound_factor

where:
  absolute_gain = 25x (authorship velocity)
  frequency = 0.15 (15% of functions)
  compound_factor = 1.3 (reusability across backends)

total_leverage = 25 × 0.15 × 1.3 = 4.9x effective leverage
```


---


## 5. dimension summary

| dimension | score | key insight |
|-----------|-------|-------------|
| leverage.authorable | 9.3/10 | one-line wrapper eliminates cache boilerplate |
| leverage.supportable | 7.8/10 | predictable pattern; cache logic isolated |
| leverage.adoptable | 8.5/10 | works with any function, any backend |

**overall leverage grade:** B+ (8.5/10)

**one-sentence summary:** with-simple-cache provides ~5x effective leverage by eliminating cache boilerplate for ~15% of functions, with high reusability across cache backends.

