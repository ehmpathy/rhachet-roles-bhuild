# research: leverage dimensions — expanding the human seed perspective

this document expands on the human's seed perspective (v0.seed.md) and compares it against cross-domain research findings from v1 (software industry) and v2 (physics, biology, finance, ecology).


---


## 1. the human's seed framework

the human identifies leverage as manifesting across multiple dimensions:

### 1.1. primary dimensions

| dimension | human's definition | example given |
|-----------|-------------------|---------------|
| **faster** | takes less time to do things | "one command, totally deterministic" |
| **simpler** | less thought cycles && word tokens required | "no need to remember or know anything" |
| **safer** | pit of success | "no risk of messing up, since there's no one in the loop" |

### 1.2. secondary dimensions

| dimension | human's definition |
|-----------|-------------------|
| **observability** | dead easy to understand why and what happened |
| **readability** | makes things LOOK simple and BE intuitive |
| **composability** | how easy it is to compose this into other usecases |
| **reusability** | how many usecases a component can be reused in |
| **evolvability** | how easy it is to evolve + refactor a component |

### 1.3. key insight from human

> "pit of success" — best practices are baked into the output; no risk of doing the wrong thing

the human emphasizes **automation + determinism + best practices = leverage**.


---


## 2. dimension analysis: faster


### 2.1. human's perspective

- "takes seconds to apply best practices (one command, totally deterministic)"
- "1 command, no brain written code, all deterministic"

### 2.2. research support

**cognitive load research confirms time savings:**

> "study upon study has found that developers spend over half of their time understanding existing code." [1]

> "developers spend approximately 30% of their time navigating and understanding large codebases." [1]

> "model-driven engineering facilitates the management of complexity across numerous teams, leading to a reported 30% reduction in development time." [1]

**platform engineering validates automation speed:**

> "provisioning time: from hours/days → seconds/minutes" [2]

> "companies using IDPs deliver updates ~40% faster" [2]

### 2.3. cross-domain alignment

**physics (v2 research):**
> mechanical advantage allows "a smaller input force to generate a larger output force" — less effort, same result

**biochemistry (v2 research):**
> enzyme catalysis achieves "rate acceleration of 10^6 or more" — same reaction, massively faster

### 2.4. synthesis

the human's "faster" dimension aligns with:
- **mechanical advantage** (physics): same output, less input force
- **catalysis** (biochemistry): same transformation, dramatically accelerated
- **automation** (software): tasks that took hours/days now take seconds

**universal pattern:** `time_before / time_after = leverage.faster`

**lever arm interpretation:**
- effort_arm = scope of automation (how much work the tool handles)
- load_arm = invocation cost (commands, setup, prerequisites)
- leverage increases when: automation scope grows OR invocation cost shrinks


---


## 3. dimension analysis: simpler


### 3.1. human's perspective

- "no need to remember or know anything"
- "the best practices are applied without any pre-reqs, skill, or knowledge required"
- "no need to remember anything or think about any details"

### 3.2. research support

**cognitive load theory directly validates this:**

> "Miller's Law (1956) suggests the average person can hold about seven (±2) pieces of information in working memory, while more recent studies by Nelson Cowan (2001) suggest this number is closer to four." [1]

> "developers experiencing high cognitive load make more errors and work less efficiently than developers who are able to minimize cognitive load." [1]

> "76% of organizations admit their software architecture's cognitive burden creates developer stress and lowers productivity." [3]

**platform engineering confirms knowledge offloading:**

> "platform engineering enables developers to focus on business logic versus infrastructure mastery." [4]

> "developers spend as little as 12.5% to 30% of their time per week writing code. platform engineering enables developers to focus on business logic versus infrastructure mastery." [4]

### 3.3. cross-domain alignment

**physics (v2 research):**
> leverage is "a rigid body to multiply the mechanical force that can be applied" — the lever handles complexity, not the user

**ecology (v2 research):**
> keystone species create "disproportionate effect on its natural environment relative to its abundance" — small input, large systemic effect

### 3.4. synthesis

the human's "simpler" dimension maps to **leverage.simpler**:
- reduce working memory requirements
- offload knowledge to the system
- eliminate prerequisite skills

**universal pattern:** `thought_cycles_before / thought_cycles_after = leverage.simpler`

**lever arm interpretation:**
- effort_arm = complexity hidden by abstraction (concepts encapsulated)
- load_arm = knowledge required to use (prerequisites, learning curve)
- leverage increases when: abstraction hides more complexity OR prerequisites approach zero


---


## 4. dimension analysis: safer (pit of success)


### 4.1. human's perspective

- "pit of success with failsafe"
- "no risk of messing up, since there's no one in the loop"
- "best practices are baked into the output; no risk of doing the wrong thing"
- "no risk of using it wrong either, because the best practices of the produced dao ensure its a pit-of-success in usage"

### 4.2. research support — pit of success

**origin and definition:**

> "The Pit of Success: in stark contrast to a summit, a peak, or a journey across a desert to find victory through many trials and surprises, we want our customers to simply fall into winning practices by using our platform and frameworks." — Rico Mariani, MS Research MindSwap, October 2003 [5]

> "it is possible to guide customers into success by creating sensible defaults that tend to be the successful path in most cases." [5]

> "when developers fail, it's usually a design problem, not a developer problem. make success inevitable through good design." [6]

**implementation strategies:**

> "analyzing failure points to identify where developers commonly make mistakes, designing around mistakes to make those mistakes harder or impossible, optimizing for common cases to make frequent operations simple and safe" [6]

### 4.3. research support — failsafe design

> "in engineering, a fail-safe is a design feature or practice that, in the event of a failure of the design feature, inherently responds in a way that will cause minimal or no harm to other equipment, to the environment or to people." [7]

> "fail-safe systems handle errors gracefully, making them better suited for mission-critical systems and volatile environments, where catastrophic failures can be devastating." [8]

> "silent failures are dangerous because they hide the truth. in critical systems, it's better to be clearly broken than silently wrong." [8]

### 4.4. cross-domain alignment

**physics (v2 research):**
> ideal mechanical advantage represents "frictionless" operation — no energy lost to failure modes

**systems thinking (v2 research):**
> meadows identifies "rules of the system" as leverage point #5 — changing rules changes outcomes systemically

**ecology (v2 research):**
> "trophic cascades" demonstrate how proper keystone placement prevents ecosystem collapse

### 4.5. synthesis

the human's "safer" dimension combines two distinct patterns:
1. **pit of success**: make correct behavior the default/easy path
2. **failsafe**: when errors occur, fail gracefully without harm

**universal pattern:** `risk_before / risk_after = leverage.safer`

**lever arm interpretation:**
- effort_arm = risk surface covered by guard rails (failure modes prevented)
- load_arm = effort to stay on the safe path (cognitive load of correct usage)
- leverage increases when: more failure modes are prevented OR correct path becomes the default path


---


## 5. dimension analysis: observability


### 5.1. human's perspective

- "helpful-errors ensures that whenever an error is thrown, it is dead easy to understand why and what happened"
- "serializes all the info safely for logs and incentivizes authors to pass in maximum context"
- "as-procedure embeds withLogTrail on every procedure, to make it easy to trace down exactly what happened"
- "can instantly see when something failed or where latency originates"
- "can easily reproduce as the full input and output is present"

### 5.2. research support — three pillars of observability

> "observability relies on three pillars of telemetry data—metrics, logs and traces—to make computing networks easier to visualize and understand." [9]

> "observability lets you understand a system from the outside by letting you ask questions about that system without knowing its inner workings. it also allows you to easily troubleshoot and handle novel problems, that is, 'unknown unknowns'." [10]

**logs:**
> "when something fails, logs are our most valuable ally as a primary forensics tool. for logs to be truly useful, they should allow us to reconstruct timelines—providing a chronological history of events, ideally across systems." [11]

**traces:**
> "tracing, as a part of debugging workflow, significantly lowers MTTR (Mean Time to Resolution)." [12]

> "distributed tracing is emerging as the critical component to having strong debuggability across your system." [12]

### 5.3. cross-domain alignment

**physics (v2 research):**
> archimedes' principle: `effort × effort_arm = load × load_arm` — the relationship is **visible and measurable**

**thermodynamics (v2 research):**
> entropy provides a measurable quantity for system state — observability of energy distribution

### 5.4. synthesis

the human identifies observability as a **leverage dimension** because:
- reduces debugging time (MTTR)
- enables root cause analysis without deep system knowledge
- makes failures "loud and clear" rather than silent

**universal pattern:** `debug_time_before / debug_time_after = leverage.observable`

**lever arm interpretation:**
- effort_arm = system state exposed by instrumentation (breadth × depth of visibility)
- load_arm = effort to access that visibility (queries, dashboards, log searches)
- leverage increases when: more state is automatically captured OR accessing it becomes trivial


---


## 6. dimension analysis: readability


### 6.1. human's perspective

- "makes things LOOK simple and BE intuitive"
- "decrease tokens required to understand and evolve"
- "decrease maintenance burden && decrease evolution cost (refactor cost)"

### 6.2. research support

**foundational research (buse & weimer):**

> "the analysis confirms the widely-held belief that humans agree significantly on what readable code looks like, but not to an overwhelming extent. this implies that there are underlying factors that influence readability of code." [13]

> "this metric correlates strongly with two traditional measures of software quality: code changes and defect reports." [13]

**readability-maintainability connection:**

> "source code readability and documentation readability are crucial for maintainability of software. some researchers identify reading code as a key activity in software maintenance, and recognize it as the most time-consuming activity among all maintenance activities." [14]

> "research confirms a negative correlation between code readability and program complexity. this means that low readability increases program complexity." [15]

**key factors affecting readability:**

> "the five strongest code constructs that positively affect code readability are: the presence of comments, spacing, while loops, meaningful names, and do-while loops." [15]

### 6.3. cross-domain alignment

**cognitive science (from v2 thermodynamics research):**
> minimal entropy systems are more predictable — readability = reduced cognitive entropy

**information theory:**
> compression reduces redundancy while preserving meaning — readable code = well-compressed meaning

### 6.4. synthesis

the human's "readability" dimension maps to **leverage.readable**:
- reduces tokens required to understand
- correlates with defect reduction
- directly impacts maintenance cost

**universal pattern:** `comprehension_time_before / comprehension_time_after = leverage.readable`

**lever arm interpretation:**
- effort_arm = intent conveyed by naming/structure (meaning density per token)
- load_arm = tokens required to parse (visual complexity, indirection)
- leverage increases when: each token carries more meaning OR fewer tokens are needed


---


## 7. dimension analysis: composability


### 7.1. human's perspective

- "how easy it is to compose this into other usecases"
- "how safe it is to do so"
- "enables a ton of transitive tools to leverage that domain knowledge to power even higher leverage behaviors"

### 7.2. research support

**component-based software development:**

> "assessing the reusability, adaptability, compose-ability and flexibility of software components is increasingly necessary due to the growing popularity of component based software development (CBSD)." [16]

> "existing metric suites such as CK, MOOD, Halstead, and McCabe's metrics measure reusability of object-oriented software systems, but they are not directly applicable to the CBSD environment because these metrics work at the class level and don't target component interfaces that define composability." [16]

### 7.3. cross-domain alignment

**physics (v2 research):**
> lever systems can be **chained** — output of one becomes input of another

**biochemistry (v2 research):**
> enzyme cascades: "cascade amplification where one enzyme activates many copies of the next"

**ecology (v2 research):**
> trophic cascades: "effects reverberate through multiple trophic levels"

### 7.4. synthesis

**critical distinction: composability vs composed leverage**

there are two related but distinct concepts:

1. **composability** (intrinsic property) = how easy is it to compose THIS tool into other contexts?
2. **composed leverage** (result) = what is the total leverage when multiple tools ARE composed?

the human emphasizes the **intrinsic composability** as a leverage dimension:
> "how easy it is to compose this into other usecases && how safe it is to do so"

this is the **potential** for leverage multiplication, not the multiplication itself.


### 7.5. measuring composability (intrinsic)

**what makes a tool composable?**

| factor | description | high composability | low composability |
|--------|-------------|-------------------|-------------------|
| **interface clarity** | how obvious is input/output? | typed, documented contracts | implicit, undocumented |
| **assumptions** | what does tool assume about environment? | minimal, explicit | many, hidden |
| **side effects** | does tool modify external state? | pure, isolated | mutates globals |
| **dependencies** | what must be present to use? | few, injected | many, hardcoded |
| **output shape** | can output feed into other tools? | standard, transformable | proprietary, opaque |

**composability formula:**

```
composability = integration_potential / integration_friction
```

where:
- **integration_potential** = how many downstream usecases can consume this tool's output
- **integration_friction** = effort required to adapt tool for each usecase (glue code, transformations, workarounds)


### 7.6. composed leverage (result)

when composable tools ARE chained, their leverage multiplies:

```
leverage.total = leverage_1 × leverage_2 × ... × leverage_n
```

**physics analogy:**
> lever systems can be **chained** — output of one becomes input of another

**biochemistry analogy:**
> enzyme cascades achieve "cascade amplification where one enzyme activates many copies of the next"

**the compounding effect:**
- each composable unit becomes a building block for higher-order leverage
- high composability enables this multiplication
- low composability blocks it (friction prevents chaining)


### 7.7. composability as leverage enabler

**key insight:** composability is the **gate** that determines whether leverage can compound.

```
leverage.composed.effective = composability_1 × composability_2 × ... × leverage.product
```

if any `composability_n` approaches zero (tool is not composable), the entire chain breaks.

**examples from human's seed:**

| tool | composability trait | why it enables chaining |
|------|--------------------|-----------------------|
| domain-objects | standard shape, typed refs | "enables transitive tools to leverage domain knowledge" |
| dynamodb-dao-generator | consumes domain-objects output | builds on prior tool's output |
| RefByUnique | composable reference type | "enables composition with declastruct daos" |

the human explicitly notes: "enables a ton of transitive tools to leverage that domain knowledge to power even higher leverage behaviors" — this is composability enabling leverage compounding.


**universal pattern:** `friction_wout / friction_with = leverage.composable`

where the intervention can be either **tool creation** or **tool update**:

| intervention | friction_wout | friction_with |
|--------------|---------------|---------------|
| **tool.create** | friction wout tool existing | friction with tool existing |
| **tool.update** | friction wout improvement applied | friction with improvement applied |

**symmetric formulas:**

```
leverage.composable.create = friction_wout_tool / friction_with_tool
leverage.composable.update = friction_before_update / friction_after_update
```

both reduce to the same core pattern:

```
leverage.composable = friction_counterfactual / friction_actual
```

**example: tool.create (domain-objects)**

```
friction_wout_tool:
  - each downstream tool (dao-generator, serializer, validator) defines own shape
  - no shared contract → N usecases × M lines of adapter code = N×M

friction_with_tool:
  - single canonical shape with `.unique`, `.updatable`, typed refs
  - N usecases × ~0 lines (just import) = N×~0

leverage.composable.create = (N × M) / (N × ~0) → very high
```

**example: tool.update (adding typed refs to domain-objects)**

```
friction_before_update:
  - downstream tools manually construct references
  - 10 usecases × 5 lines of ref-building code = 50

friction_after_update:
  - downstream tools use RefByUnique<typeof X>
  - 10 usecases × 1 line = 10

leverage.composable.update = 50 / 10 = 5x
```

**key insight:**

> leverage.composable = integration friction PREVENTED by the intervention (create or update)

the leverage scales with:
1. **downstream graph size** — more consumers = more friction prevented
2. **friction reduction per consumer** — cleaner interface = less adapter code each

**equivalent formulation:**

```
composability = integration_potential / integration_friction
```

**lever arm interpretation:**
- effort_arm = integration potential (downstream usecases this tool can feed)
- load_arm = integration friction (glue code, adapters, assumptions to work around)
- leverage increases when: tool fits more contexts OR requires less adaptation per context

**secondary pattern (when composed):**

```
leverage.total = leverage_1 × leverage_2 × ... × leverage_n
```

**unique property**: composability is the **multiplier coefficient** — it determines HOW MUCH of each tool's leverage transfers to the chain. high leverage.composable means the tool can participate in more leverage chains with less friction.


---


## 8. dimension analysis: reusability


### 8.1. human's perspective

- "how many usecases a component can be reused in (via composition or direct usage)"

### 8.2. research support

**reuse metrics taxonomy (frakes & terry):**

> "reuse models can be categorized into six types: reuse cost-benefits models (economic analysis), maturity assessment models, amount of reuse metrics (tracking percentages), failure modes analysis (identifying impediments), reusability metrics (indicating likelihood of reuse), and reuse library metrics." [17]

**modern approaches:**

> "the file-level PUA (Public Undocumented API) metric is the most important factor influencing software reuse." [18]

> "researchers have proposed metrics suites for measuring reusability of black-box components based on limited information obtainable without source code, defining five metrics for measuring understandability, adaptability, and portability with confidence intervals based on statistical analysis." [16]

### 8.3. cross-domain alignment

**physics (v2 research):**
> "ideal mechanical advantage" is independent of load — a lever works for any load within its capacity

**economics (v2 research):**
> economies of scale: fixed costs amortized across more units

### 8.4. synthesis

reusability measures **leverage scope**:
- how many problems does this solution address?
- amortized development cost across usecases

**universal pattern:** `leverage.reusable = usecases_served / development_investment`

**lever arm interpretation:**
- effort_arm = usecases addressed by the solution (problem space coverage)
- load_arm = development investment (build cost + maintenance cost)
- leverage increases when: solution addresses more usecases OR amortized cost per usecase shrinks
- **unique property**: reusability extends the effort arm across **parallel** domains (horizontal scaling)


---


## 9. dimension analysis: evolvability


### 9.1. human's perspective

- "how easy it is to evolve + refactor a component"
- "domain knowledge evolves constantly and our software must evolve with it"
- "goes hand in hand with experimentation"

### 9.2. research support

**technical debt and evolvability:**

> "technical debt presents an actual or contingent liability whose impact is limited to internal system qualities, primarily maintainability and evolvability." [19]

> "architectural technical debt (ATD) is incurred by design decisions that intentionally or unintentionally compromise system-wide quality attributes, particularly maintainability and evolvability." [20]

**evolvability assurance practices:**

> "the goal of analytical activities is to identify evolvability-related issues in the system, i.e., to evaluate or quantify evolvability. this includes manual techniques like code review or scenario-based analysis but also tool-supported static or dynamic analysis with e.g. metrics." [21]

> "the primary constructive activity is code-level or architectural refactoring. however, adhering to evolvability-related principles and guidelines or using evolvability-related design patterns during software evolution is also a constructive – and proactive – form of evolvability assurance." [21]

**evolution velocity:**

> "fast moving markets and the age of digitalization require that software can be quickly adapted or extended with new features." [21]

### 9.3. cross-domain alignment

**biology (v2 research):**
> evolutionary fitness is "probability of survival and reproduction" — evolvability = adaptive capacity

**ecology (v2 research):**
> ecosystem resilience depends on "keystone species" that enable adaptation

### 9.4. synthesis

evolvability measures **future leverage preservation**:
- can the system adapt to new requirements?
- how much does change cost over time?

**universal pattern:** `leverage.evolvable = change_capacity / change_cost`

**lever arm interpretation:**
- effort_arm = change isolated by modularity boundaries (blast radius containment)
- load_arm = refactoring cost per change (coupling, dependencies to update)
- leverage increases when: boundaries isolate more change OR refactoring requires fewer touchpoints
- **unique property**: evolvability preserves leverage **over time** — the arm doesn't shorten as requirements evolve


---


## 10. human framework vs research frameworks


### 10.1. alignment analysis

| human dimension | research equivalent | v1/v2 alignment |
|-----------------|--------------------|--------------------|
| faster | velocity, throughput | ✓ DORA deployment frequency, lead time |
| simpler | cognitive load reduction | ✓ platform engineering, IDE metrics |
| safer | pit of success, failsafe | ✓ DORA stability (CFR, MTTR) |
| observability | three pillars (logs, metrics, traces) | ✓ SPACE efficiency & flow |
| readability | code comprehension metrics | ✓ maintainability research |
| composability | component interfaces, CBSD | ✓ technical leverage ratio |
| reusability | reuse metrics (frakes & terry) | ✓ borrowed assets (ICSE 2021) |
| evolvability | technical debt, refactoring | ✓ maintenance leverage |

### 10.2. unique contributions from human seed

the human's framework contributes several insights **not prominent** in existing research:

1. **automation + determinism emphasis**: "totally deterministic" — research on determinism is sparse outside safety-critical systems

2. **zero-prerequisite design**: "no need to remember or know anything" — goes beyond cognitive load to **zero knowledge required**

3. **transitive tool enablement**: "enables a ton of transitive tools to leverage that domain knowledge" — explicit recognition of **leverage compounding**

4. **pit of success as leverage**: research treats pit of success as UX/API design; human frames it as **leverage mechanism**


### 10.3. gaps in human framework

compared to v1/v2 research, the human's framework does not explicitly address:

1. **conservation laws**: leverage always has tradeoffs (v2 physics research)
2. **bound limits**: leverage cannot be infinite (v2 thermodynamics)
3. **risk-reward tradeoffs**: high leverage = high vulnerability exposure (v1 ICSE 2021)


---


## 11. proposed unified model


### 11.1. leverage as input efficiency

all human dimensions reduce to **input efficiency**:

```
leverage = output_equivalent / input_required
```

| dimension | numerator (output) | denominator (input) |
|-----------|-------------------|---------------------|
| faster | same functionality | less time |
| simpler | same functionality | less thought cycles |
| safer | same functionality | less risk exposure |
| observable | same debugging outcome | less investigation time |
| readable | same comprehension | less reading effort |
| composable | same integration | less glue code |
| reusable | same solution | less development |
| evolvable | same adaptation | less refactor cost |


### 11.2. leverage dimensions as orthogonal axes

the human's dimensions form a **multi-dimensional leverage space**:

```
leverage.total = f(
  leverage.faster,
  leverage.simpler,
  leverage.safer,
  leverage.observable,
  leverage.readable,
  leverage.composable,
  leverage.reusable,
  leverage.evolvable
)
```


### 11.3. the lever arm analogy

in physics, leverage is determined by **arm length**:

```
mechanical_advantage = effort_arm_length / load_arm_length
```

archimedes' law: `effort × effort_arm = load × load_arm`

the key insight: **where you apply force matters as much as how much force you apply.**

a longer effort arm means less force required. the arm acts as the **mechanism** that translates small inputs into large outputs.

**mapping arm length to software dimensions:**

| dimension | what is the "arm"? | what determines "arm length"? |
|-----------|-------------------|------------------------------|
| **faster** | the automation tool | how much work per invocation (scope of automation) |
| **simpler** | the abstraction layer | how much complexity hidden per interface |
| **safer** | the guard rails / defaults | how much risk surface covered per safeguard |
| **observable** | the instrumentation | how much system state exposed per trace/log |
| **readable** | the naming/structure conventions | how much intent conveyed per token |
| **composable** | the interface contract | how many integration points per component |
| **reusable** | the generalized solution | how many usecases addressed per implementation |
| **evolvable** | the modularity boundaries | how much change isolated per boundary |

**the universal lever formula applied to software:**

```
leverage = (output_force / input_force) = (effort_arm / load_arm)
```

translates to:

```
software_leverage = (outcome_achieved / effort_invested) = (mechanism_scope / mechanism_cost)
```

where:
- **mechanism_scope** = how much the tool/abstraction/pattern handles (analogous to effort_arm length)
- **mechanism_cost** = what it takes to use/maintain the mechanism (analogous to load_arm length)

**example: declapract (from human's seed)**

```
effort_arm = "all best practices across entire codebase" (scope: massive)
load_arm = "one command invocation" (cost: minimal)
leverage = massive / minimal = very high
```

**example: helpful-errors (from human's seed)**

```
effort_arm = "full context serialized for debugging" (scope: complete observability)
load_arm = "pass metadata to error constructor" (cost: one argument)
leverage = complete / one = very high
```

**the arm length principle:**

> to increase leverage, either **lengthen the effort arm** (increase what the mechanism handles) or **shorten the load arm** (decrease what's required to use it).

this is why the human's examples emphasize:
- "one command" = short load arm
- "all best practices applied" = long effort arm
- "no prerequisites required" = short load arm
- "enables transitive tools" = extending effort arm further


### 11.4. cross-domain validation

every human dimension maps to a cross-domain principle:

| human dimension | physics | biology | economics |
|-----------------|---------|---------|-----------|
| faster | mechanical advantage | enzyme catalysis | time value of money |
| simpler | lever (offload force) | keystone species | economies of scope |
| safer | failsafe mechanisms | homeostasis | hedging, insurance |
| observable | measurable equilibrium | biomarkers | financial auditing |
| readable | minimal entropy | genetic readability | contract clarity |
| composable | lever chains | cascade amplification | financial instruments |
| reusable | universal mechanisms | generalist enzymes | economies of scale |
| evolvable | adaptive systems | evolutionary fitness | optionality |


---


## 12. citations


[1] [Measuring the cognitive load of software developers - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S095058492100046X)

[2] [Platform Engineering Reduces Cognitive Load - The New Stack](https://thenewstack.io/platform-engineering-reduces-cognitive-load-and-raises-developer-productivity/)

[3] [Reducing Cognitive Load for Software Developers - CAST Software](https://www.castsoftware.com/pulse/alleviating-the-cognitive-load-on-software-developers)

[4] [Platform Engineering Key Driver of Developer Productivity - CloudBees](https://www.cloudbees.com/newsroom/survey-platform-engineering-key-driver-developer-productivity)

[5] [The Pit of Success - Microsoft Learn](https://learn.microsoft.com/en-us/archive/blogs/brada/the-pit-of-success)

[6] [Pit of Success in Software Development - GlassThought](https://www.glassthought.com/notes/jno6ha3xm4enkn5ctzpex9d/)

[7] [Fail-safe - Wikipedia](https://en.wikipedia.org/wiki/Fail-safe)

[8] [Fail-Safe by Design - Medium](https://medium.com/@jusuftopic/fail-safe-by-design-how-to-build-systems-that-break-gracefully-081e5802d350)

[9] [Three Pillars of Observability - IBM](https://www.ibm.com/think/insights/observability-pillars)

[10] [Observability primer - OpenTelemetry](https://opentelemetry.io/docs/concepts/observability-primer/)

[11] [A Software Engineer's Guide to Observability - Blueground](https://engineering.theblueground.com/a-software-engineers-guide-to-observability-part-1-logging/)

[12] [Observability and Tracing - Sentry](https://blog.sentry.io/observability-and-tracing-how-to-improve-your-debugging-workflow/)

[13] [A Metric for Software Readability - Buse & Weimer](https://web.eecs.umich.edu/~weimerw/p/weimer-issta2008-readability.pdf)

[14] [An empirical study on software understandability - Springer](https://link.springer.com/article/10.1007/s10664-023-10396-7)

[15] [Readability Metrics - IJCTT Journal](https://www.ijcttjournal.org/archives/ijctt-v46p101)

[16] [A metrics suite for measuring reusability - IEEE Xplore](https://ieeexplore.ieee.org/document/1232469/)

[17] [Software Reuse: Metrics and Models - ResearchGate](https://www.researchgate.net/publication/220565923_Software_Reuse_Metrics_and_Models)

[18] [Measuring the Reusability of Software Components - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0164121219301979)

[19] [Technical Debt: From Metaphor to Theory and Practice - IEEE](https://dl.acm.org/doi/10.1109/MS.2012.167)

[20] [An empirically developed method to aid decisions on architectural technical debt - ACM](https://dl.acm.org/doi/10.1145/2889160.2889224)

[21] [Industry practices and challenges for the evolvability assurance of microservices - Springer](https://link.springer.com/article/10.1007/s10664-021-09999-9)

[22] [Deterministic build systems - reproducible-builds.org](https://reproducible-builds.org/docs/deterministic-build-systems/)

[23] [Reproducibility of computational workflows - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC6103790/)

[24] [The Rico Mariani - The Pit of Success - Medium](https://ricomariani.medium.com/the-pit-of-success-cfefc6cb64c8)


---


## 13. summary


### 13.1. human's framework validated

the human's seed framework identifies **8 leverage dimensions** that are each supported by academic and industry research:

1. **faster** — confirmed by DORA metrics, platform engineering, model-driven engineering
2. **simpler** — confirmed by cognitive load theory, miller's law, platform engineering
3. **safer** — confirmed by pit of success (rico mariani 2003), failsafe engineering
4. **observable** — confirmed by three pillars (logs, metrics, traces), MTTR research
5. **readable** — confirmed by buse & weimer readability metrics, maintainability research
6. **composable** — confirmed by CBSD research, interface metrics
7. **reusable** — confirmed by frakes & terry reuse taxonomy, ICSE technical leverage
8. **evolvable** — confirmed by technical debt research, architectural debt studies


### 13.2. human's unique insight

the human's key contribution is framing **all dimensions as leverage** — as multiplicative effects on input efficiency:

> leverage is always: `effort_before / effort_after`

this unifying formula connects:
- physics (mechanical advantage)
- biology (enzyme catalysis, keystone species)
- economics (ROI, economies of scale)
- software engineering (all 8 dimensions)


### 13.3. the lever arm principle

from physics, leverage depends on **arm length**:

```
mechanical_advantage = effort_arm / load_arm
```

in software, this translates to:

```
software_leverage = mechanism_scope / mechanism_cost
```

**two ways to increase leverage:**
1. **lengthen the effort arm** — increase what the mechanism handles (scope, coverage, power)
2. **shorten the load arm** — decrease what's required to use it (prerequisites, setup, complexity)

the human's examples consistently maximize both:
- declapract: massive scope (all best practices) + minimal cost (one command)
- helpful-errors: complete observability + minimal effort (one constructor arg)
- domain-objects: full domain modeling + simple declaration syntax

**the lever arm table:**

| dimension | effort arm (lengthen this) | load arm (shorten this) | formula |
|-----------|---------------------------|------------------------|---------|
| faster | automation scope | invocation cost | `time_before / time_after` |
| simpler | complexity hidden | knowledge required | `thought_cycles_before / thought_cycles_after` |
| safer | failure modes prevented | effort to be correct | `risk_before / risk_after` |
| observable | state visibility | query complexity | `debug_time_before / debug_time_after` |
| readable | meaning per token | tokens to parse | `comprehension_time_before / comprehension_time_after` |
| composable | integration potential | integration friction | `integration_potential / integration_friction` |
| reusable | problems addressed | development investment | `usecases_served / development_investment` |
| evolvable | change isolated | refactoring touchpoints | `change_capacity / change_cost` |

**note on composability:** composability is measured as an **intrinsic property** of the tool itself (how easy to compose), not the result of composition. when prioritizing work to improve composability, measure the delta:

```
gain.composable = composability_after - composability_before
                = (potential_after / friction_after) - (potential_before / friction_before)
```

this enables prioritizing work that increases a tool's composability as a measurable leverage investment.


### 13.4. recommended next step

use these 8 dimensions as **measurable leverage factors** in the dispatcher's prioritization system, where each behavior can be scored on how much it improves each dimension.

