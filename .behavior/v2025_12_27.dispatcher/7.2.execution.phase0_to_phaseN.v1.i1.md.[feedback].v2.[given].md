# blocker.1

> review the briefs on leverage against .behavior/v2025_12_27.d
  ispatcher/3.1.research.domain.leverage.v9.i1.md again; looks
  like the briefs mistakenly imply that LEVERS are LEVERAGE;
  udpate all the briefs. also, we should have atleast one brief
  created from the contents of .behavior/v2025_12_27.dispatche
  r/3.1.research.domain.leverage.v9.i1.md (if not multiple);
  ensure all the briefs that speak about leverage conform to
  the latest outcome distilled in .behavior/v2025_12_27.dispatc
  her/3.1.research.domain.leverage.v9.i1.md


---

# blocker.2


> create the briefs that were explicitly requested in
.behavior/v2025_12_27.dispatcher/2.criteria.md

 next, create briefs based on
 - .behavior/v2025_12_27.dispatcher/3.1.research.domain.leverage.v9.i1.md and
 - .behavior/v2025_12_27.dispatcher/2.criteria.blackbox.md

create them based on this format; https://github.com/ehmpathy/rhachet-roles-bhrain/tree/main/src/domain.roles/thinker/briefs/knowledge

use the gh api to access rhachet-roles-bhrain docs

stick them under domain.roles/dispatcher/briefs/terms/

---

# blocker.3

enumerate the domain.operations that invoke a brain.repl

then, identify what briefs those brain.repls should have in order to maximize how well they do their jobs

then research and add the briefs

ensure they conform to the standards and language adopted already in the briefs from .behavior/v2025_12_27.dispatcher/3.1.research.domain.leverage.v9.i1.md and already created in domain.roles/dispatcher/briefs/terms

---

# blocker.4

take a look at all of the briefs within
src/domain.roles/dispatcher/briefs/practices/measure ; are
they coherent? do their filepaths declare a sequential story
that builds uopn eachother? update the paths to use
measure101.metric.1.gain.xyz, measure101.metric.2.cost.xyz,
measure101.metric.3.effect.xyz in order to make the briefs
form a sequential knowledge journey that can be used to learn
 from


---

# blocker.5

update the briefs, criteria, and implementation, and
  blueprint to clarify taht the dollar amounts should be
  estimated PER WEEK; they should all be estimated as RATES;
  yieldage should be measured PER WEEK TOO!

measured.gain, cost, leverage, yieldage, attend, expend

all measured per week

everything is always a $/week rate

each of these


- .behavior/v2025_12_27.dispatcher/2.criteria.blackbox.md
- .behavior/v2025_12_27.dispatcher/2.criteria.md
- .behavior/v2025_12_27.dispatcher/3.1.research.domain._.v1.i1.md
- .behavior/v2025_12_27.dispatcher/3.1.research.domain.leverage.v9.i1.md
- .behavior/v2025_12_27.dispatcher/3.2.distill.domain._.v2.i1.md

$/wk

and all of the briefs within src/domain.roles/dispatcher/briefs/practices/measure

and the implmentation


note, terms like gain is declared in /wk units already, so no need to say  gain/wk

 only need to specify units when values are specified

 combined: `gain(+$/wk) = leverage + yieldage`; again, no need
  to specify $/wk unless the value is there


-----

# blocker.6

we need to amortize the cost over a cost horizon when calculating cost from upfront costs (rather than recurrent costs)

make the cost.horizon configurable in dispatch config

specifically, update the briefs + criteria + blueprint + implementation with this

- .behavior/v2025_12_27.dispatcher/2.criteria.md
- .behavior/v2025_12_27.dispatcher/3.1.research.domain._.v1.i1.md
- .behavior/v2025_12_27.dispatcher/3.1.research.domain.leverage.v9.i1.md
- .behavior/v2025_12_27.dispatcher/3.2.distill.domain._.v2.i1.md

$/wk

and all of the briefs within src/domain.roles/dispatcher/briefs/practices/measure

and the implmentation

make the default horizon = 24 weeks (1 half)

> this should apply both to expend and attend upfront vs
recurrent

-----

# blocker.7

were the recent changes to measure in units of $/wk and amortize upfront cost.attend and cost.expend propogated to the implementations?

-----

# blocker.8

where are the integration tests that run the brain.repl against example repos for measure and verify that gain and cost are calculated as expected and produce the expected relative priorities for a set of behaviors?

should literally invoke the brain.repl and prove it produces the expected output

the test should be careful to be written to not be flakey


----

# blocker.9

where are the integration tests for each domain operation? we should prove that the brain.repl works for each that uses it, .then.repeatedly({ criteria: process.env.CI ? 'SOME' : 'EVERY', attempts: 3 })


----

# dispatched

emit your plan into
.behavior/v2025_12_27.dispatcher/7.2.execution.phase0_to_phaseN.v1.i1.md.[feedback].v2.p3.[taken].by_robot.md

and then execute it

----

# blocker.10


> transitive should be COMPUTED, not IMAGINED via brain.repl;
 update your plan and the implementation; only direct should
be imagined via brain.repl

● Understood - transitive values should be computed deterministically from deptraced data, only direct values use brain.repl. Updating the implementation:

> also, update the brain.repl interface to be
brain.repl.imagine

-----

# blocker.11



> what briefs should brains have loaded for these skills, when
 they invoke brain.repl? ensure that we declare the briefs under
domain.roles/decomposer/briefs/ - then ensure that we
pass those through in the context of the brain.repl.imagine() requests


---

# blocker.12

> your tests should run fast enough to run in foreground
use a --rapid mode via haiku for integration tests, in order
to support faster boundary integration test verification


> also, rename operations which leverage brain.repl to be
  prefixed with imagine instead of compute

---

# blocker.13

why does that test take so long? if there's multiple
  brain.repl invocations, split each test into its own
  integration.test.ts file (via prefix) so that tehy run in
  parallel

---

# blocker.14

> first, you should have integration tests written for each
boundary operation (each operation that starts with term
imagine*); then, getOneBehaviorMeasured should invoke each of
 those imagine operations in parallel, so the total walltime
is still fast - and the getOneBehaviorMeasured integration
tests should prove that its fast


