# vision: behavior-guard-self-review

## the outcome world

### before

a brain completes a stone and marks it as passed. the guard runs peer reviews via external tools (e.g., `rhx review`). if blockers are found, the brain iterates. however:

- **self-reflection is implicit** — the brain may not have paused to verify its own work before peer review starts
- **junior oversight gaps** — when a less experienced brain works on a stone, they may miss obvious issues that a senior would catch
- **wasted peer review cycles** — peer reviews surface issues that could have been caught via simple self-checks

### after

guards now support **self-reviews** (`reviews.self`) alongside peer reviews (`reviews.peer`). before peer review runs, the brain is prompted with structured self-reflection questions:

- "did you complete all that was requested?"
- "are there any unquestioned assumptions?"
- "could this be simpler?"

this catches issues early, reduces peer review iterations, and builds a habit of self-verification.

### the "aha" moment

> "wait, the guard just asked me if i'm sure all the criteria were covered — and i realized i missed one. fixed it before the peer review even ran. no wasted iteration."

## user experience

### usecases

1. **self-check before peer review** — brain verifies own work against stone requirements
2. **junior oversight** — prompts designed for junior brains to catch common gaps
3. **simplification check** — prompts to identify over-complication

### contract inputs & outputs

**guard file format** (yaml):
```yaml
reviews:
  self:
    - slug: all-done-self
      say: |
        are you sure all was completed? review the blueprint and criteria.
        emit a report into the .behavior dir on gaps detected. then, fix all gaps.
    - slug: all-done-junior
      say: "@briefs/self-review.all-done-junior.md"
  peer:
    - rhx review --rules .agent/**/rules/*.md --paths src/**/*.ts
```

**self-review slugs**:
- `slug` — identifier for the self-review (used for tracker)
- `say` — the prompt content (inline or `@path/to/brief.md` reference)

**outputs**:
- self-review artifacts emitted to `.route/{stone}.guard.review.self.{slug}.md`
- gap reports emitted to `.behavior/` directory when issues detected

### what it looks like

**execution stone guard**:
```yaml
reviews:
  self:
    - slug: all-done-self
      say: |
        are you sure all was completed? review the blueprint and criteria.
        emit a report into the .behavior dir on gaps detected. then, fix all gaps.
    - slug: all-done-junior
      say: |
        a junior recently worked on this branch. review against the blueprint
        and criteria and flag any gaps detected. emit a report into the
        .behavior dir if any and fix all gaps.
```

**blueprint stone guard**:
```yaml
reviews:
  self:
    - slug: all-done-self
      say: |
        are you sure all of the criteria and vision was covered?
        emit a report into the .behavior dir on gaps detected. then, fix all gaps.
    - slug: all-done-junior
      say: |
        a junior recently worked on this blueprint. review against the criteria
        and vision and flag any gaps detected. emit a report into the
        .behavior dir if any and fix all gaps.
    - slug: all-simple-junior
      say: |
        a junior recently worked on this blueprint. are there any parts that
        are needlessly over complicated? any opportunities to simplify yet
        still fulfill the full vision and criteria?
```

**criteria stone guard**:
```yaml
reviews:
  self:
    - slug: all-real-junior
      say: |
        a junior recently worked on this criteria. are there any unquestioned
        assumptions the junior took as requirements? any of these need to
        be asked to the wisher instead?
```

### timeline

1. brain works on stone artifact
2. brain marks stone as passed (`route.stone.set --as passed`)
3. guard activates
4. **self-reviews run first** — brain reflects on each `reviews.self` prompt
5. if issues found → brain fixes and re-marks
6. **peer reviews run second** — external tools validate artifacts
7. judges evaluate pass/fail

## mental model

### how users describe this

> "it's like the guard asks you 'are you sure?' before it calls in the reviewers. catches silly mistakes before they waste everyone's time."

### analogies

- **self-review is like a pre-flight checklist** — pilots verify systems before takeoff, not after
- **self-review is like 'did you check your pockets?'** — simple prompt that catches items left behind
- **self-review is like a junior who asks their rubber duck** — the act of an explanation forces clarity

### terminology

| user term | system term | definition |
|-----------|-------------|---------|
| self-check | `reviews.self` | prompts for self-reflection |
| peer review | `reviews.peer` | external validation commands |
| guard | `.guard` file | configuration for stone validation |
| slug | self-review identifier | used for tracker and artifacts |

## evaluation

### how well does it solve the goals?

| goal | solved? | how |
|------|---------|-----|
| catch gaps before peer review | ✅ | explicit prompts force verification |
| junior oversight | ✅ | tailored prompts for common junior gaps |
| reduce wasted iterations | ✅ | self-fix before peer review runs |
| simplification check | ✅ | explicit prompt to question complexity |

### pros

- **low overhead** — yaml config, no new tools
- **composable** — mix self and peer reviews as needed
- **explicit prompts** — forces specific reflection vs vague "review your work"
- **@path references** — reuse prompts across guards via brief files
- **slug tracker** — each self-review tracked independently

### cons

- **relies on brain compliance** — brain must actually reflect, not just skip
- **no enforcement** — self-reviews are prompts, not automated checks
- **multiline yaml** — requires careful format for inline say

### edgecases

| edgecase | pit of success |
|----------|----------------|
| brain ignores self-review | peer review still runs as fallback |
| self-review prompt is too long | use `@path/to/brief.md` reference |
| need same prompt across guards | brief files enable reuse |
| no peer review needed | `reviews.peer: []` or omit peer section |
| backwards compatibility | flat `reviews:` array still works (all peer) |

## awkwardness uncovered

1. **"junior" framing** — is "junior" the right term? alternatives: "prior-work", "fresh-eyes", "outsider"
   - **recommendation**: keep "junior" — it's clear and sets the right mindset

2. **report location** — `.behavior/` dir vs `.route/` dir for gap reports?
   - **recommendation**: `.behavior/` for visibility in the behavior's own directory

3. **multiline yaml** — `say: |` syntax requires careful indentation
   - **recommendation**: prefer `@brief` references for longer prompts
