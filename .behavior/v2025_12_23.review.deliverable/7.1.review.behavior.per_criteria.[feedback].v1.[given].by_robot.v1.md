# generated by: review.deliverable --for.behavior review.deliverable --against criteria

---

## summary

the implementation in `review.deliverable.sh` and `review.deliverable.integration.test.ts` satisfies most of the declared criteria in `2.criteria.md`. the skill correctly discovers behaviors by name, resolves declaration files, invokes claude code, and emits structured feedback. test fixtures cover the key scenarios.

---

# blocker.1 = missing test for "latest major version selection" with multiple blueprint versions

**location**: `2.criteria.md` lines 46-48 and 144-145

**criteria**:
```
given('--against blueprint')
  when('multiple blueprint versions exist (e.g., v1.i1, v2.i3, v3.i2)')
    then('latest major version is selected (v3.i2 over v2.i3)')
```

and from usecase.6:
```
given('test suite with example.repo fixtures')
  when('fixture has behavior with multiple blueprint versions')
    then('test proves skill selects latest major version')
```

**finding**: the `valid-behavior` fixture has both `3.3.blueprint.v1.i1.md` and `3.3.blueprint.v2.i1.md`, but the test in `[t0] --against blueprint` does not explicitly verify that `v2` was selected over `v1`. the test only checks that a feedback file is created - it does not assert which blueprint version was passed to claude in the prompt.

**fix**: add an assertion that the log file (`$LOG_DIR/input.prompt.md`) or feedback file contains reference to the v2 blueprint (e.g., `3.3.blueprint.v2.i1.md`), or add a separate test case that explicitly validates version selection.

---

# blocker.2 = missing test for roadmap target

**location**: `2.criteria.md` lines 54-58

**criteria**:
```
given('--against roadmap')
  when('multiple roadmap versions exist')
    then('latest major version is selected')
  when('no roadmap exists')
    then('skill fails fast with clear error')
```

**finding**: no test case exercises the `--against roadmap` target. the `valid-behavior` fixture includes `4.1.roadmap.v1.i1.md`, but there is no test that invokes `--against roadmap` to prove the resolution works. additionally, there is no test for the "missing roadmap" error path.

**fix**: add test cases:
1. `when('[tN] --against roadmap')` for the valid-behavior fixture
2. a fixture or case for missing roadmap that proves the skill fails fast

---

# blocker.3 = missing test for combined targets with missing file

**location**: `2.criteria.md` lines 60-65

**criteria**:
```
given('--against with multiple targets (e.g., wish,vision,criteria)')
  when('any specified target does not exist')
    then('skill fails fast identifying the missing target')
```

**finding**: the test for `--against wish,vision,criteria` uses the `valid-behavior` fixture where all files exist. there is no test proving that if one target in a comma-separated list is missing, the skill fails fast and identifies which target is missing.

**fix**: add a test case with a fixture where, e.g., `--against wish,vision,criteria` is called but `1.vision.md` is missing, proving the error message identifies "vision" as the missing target.

---

# nitpick.1 = test fixture directory naming convention

**location**: `src/domain.roles/behaver/skills/.test/assets/example.repo/`

**finding**: the fixture directories use descriptive names like `valid-behavior`, `missing-criteria`, `ambiguous-behavior`, `valid-behavior-with-blocker`. this is good. however, per the briefs (`.briefs/patterns/code.prod.repo.structure/best-practice/dot-test-and-dot-temp.md`), the recommendation is to use `.test/` directories, and the current path uses `.test/assets/example.repo/`. this is compliant.

**verdict**: compliant, no action needed.

---

# nitpick.2 = prompt asks for stdout output but interactive mode doesn't capture it

**location**: `review.deliverable.sh` lines 279-280

**finding**: in interactive mode (`--interactive`), the script runs `claude --print "$PROMPT"` but does not capture the output to a file. the prompt instructs claude to "output your review feedback directly to stdout" but in interactive mode this output is not written to the feedback file.

**suggestion**: consider clarifying that `--interactive` mode is for live user observation and the feedback file won't be written, OR automatically capture stdout even in interactive mode via tee.

---

# nitpick.3 = test timeout could be documented

**location**: `review.deliverable.integration.test.ts` lines 65, 95, 241

**finding**: the tests use `{ timeout: 120000 }` (2 minutes) for claude invocations, which is reasonable. the comment on line 66 (`// 2 min timeout for claude`) is only on one instance. consider adding this comment consistently or documenting the timeout rationale in the test file header.

---

# nitpick.4 = shell script missing shebang consistency check

**location**: `review.deliverable.sh` line 1

**finding**: the shebang is `#!/usr/bin/env bash` which is correct. however, the script uses `set -euo pipefail` on line 30 which is good practice. no issue here, just noting the script follows fail-fast conventions.

**verdict**: compliant, no action needed.

---

## verdict

- **3 blockers** must be addressed before merge
- **4 nitpicks** are suggestions for improvement

the core functionality is implemented correctly. the primary gaps are in test coverage for version selection, roadmap targets, and combined targets with partial files missing.
