Now I have all the information needed for my review. Here is my feedback:

---

# generated by: review.deliverable --for.behavior review.deliverable --against criteria

---

## summary

the implementation in `review.deliverable.sh` and `review.deliverable.integration.test.ts` covers the majority of criteria declared in `2.criteria.md`. the skill correctly discovers behaviors by name (usecase.1), resolves declaration files (usecase.2), invokes claude code with proper prompts (usecase.3), emits feedback files (usecase.4), and validates required arguments (usecase.5). test fixtures cover most key scenarios (usecase.6).

---

# blocker.1 = no explicit test that latest major version is selected for blueprints

**location**: `2.criteria.md:46-48` and `2.criteria.md:144-145`

**criteria**:
```
given('--against blueprint')
  when('multiple blueprint versions exist (e.g., v1.i1, v2.i3, v3.i2)')
    then('latest major version is selected (v3.i2 over v2.i3)')
```

```
given('test suite with example.repo fixtures')
  when('fixture has behavior with multiple blueprint versions')
    then('test proves skill selects latest major version')
```

**finding**: the `valid-behavior` fixture has both `3.3.blueprint.v1.i1.md` and `3.3.blueprint.v2.i1.md`. the test `[t0] --against blueprint` verifies a feedback file is created, but does not assert that `v2.i1` was selected over `v1.i1`. the criteria explicitly require proving the version selection behavior.

**fix**: add assertion checking `$LOG_DIR/input.prompt.md` or feedback content for `3.3.blueprint.v2.i1.md` reference, or add dedicated test proving version sorting works correctly.

---

# blocker.2 = no test coverage for `--against roadmap` target

**location**: `2.criteria.md:54-58`

**criteria**:
```
given('--against roadmap')
  when('multiple roadmap versions exist')
    then('latest major version is selected')
  when('no roadmap exists')
    then('skill fails fast with clear error')
```

**finding**: the `valid-behavior` fixture includes `4.1.roadmap.v1.i1.md`, but no test invokes `--against roadmap`. neither the success path (roadmap resolution) nor the failure path (missing roadmap) are tested.

**fix**: add test cases:
1. `[tN] --against roadmap` on valid-behavior fixture
2. fixture or case for missing roadmap that proves fail-fast behavior

---

# blocker.3 = no test for combined targets where one target is missing

**location**: `2.criteria.md:60-65`

**criteria**:
```
given('--against with multiple targets (e.g., wish,vision,criteria)')
  when('any specified target does not exist')
    then('skill fails fast identifying the missing target')
```

**finding**: the test `[t1] --against wish,vision,criteria` uses `valid-behavior` fixture where all files exist. no test proves that when one target in a comma-separated list is missing, the skill fails fast with the specific missing target identified.

**fix**: add test using `missing-criteria` fixture (which only has `0.wish.md`) with `--against wish,vision,criteria` and assert error identifies `vision` as missing.

---

# nitpick.1 = interactive mode does not write feedback file

**location**: `review.deliverable.sh:282`

**finding**: in interactive mode, `claude --print "$PROMPT"` is invoked but output is not captured to the feedback file. the prompt instructs claude to "output your review feedback directly to stdout" but this goes only to terminal, not to `7.1.review.behavior.per_{targets}.[feedback]...md`.

**suggestion**: either document this as expected behavior (interactive = for observation only), or use `tee` to capture output even in interactive mode.

---

# nitpick.2 = output filename differs slightly from criteria spec

**location**: `2.criteria.md:93-99` vs `review.deliverable.sh:202`

**criteria spec**:
```
7.1.review.behavior.per_{target}.[feedback].v1.[given].by_robot.v1.md
```

**implementation**:
```bash
OUTPUT_FILE="$BEHAVIOR_DIR/7.1.review.behavior.per_${TARGETS_SLUG}.[feedback].[given].by_robot.v${TIMESTAMP}.md"
```

**finding**: criteria says `[feedback].v1.[given]` but implementation has `[feedback].[given]` (missing version). also, implementation uses dynamic timestamp (`v${TIMESTAMP}`) instead of static `v1`. the timestamp approach is arguably better for traceability but differs from the literal criteria wording.

**suggestion**: either update criteria to reflect timestamp approach, or update implementation to match literal spec. the timestamp approach is better for idempotency and is a reasonable deviation.

---

# nitpick.3 = test timeouts could be consistent

**location**: `review.deliverable.integration.test.ts:66, 95, 241`

**finding**: tests use `{ timeout: 120000 }` (2 min) for claude invocations. only one instance has a comment explaining this. consider adding header comment documenting timeout rationale.

---

# nitpick.4 = test coverage could verify prompt contents for usecase.3

**location**: `2.criteria.md:70-76`

**criteria**:
```
given('resolved behavior directory and declaration file')
  when('skill invokes claude code')
    then('prompt includes path to declaration file')
    then('prompt includes instruction to review implementation against declaration')
    then('prompt includes instruction to emit BLOCKERs and NITPICKs')
    then('prompt includes reference to feedback template format')
```

**finding**: the tests verify exit status and output file creation, but do not assert the prompt contents. the implementation writes to `$LOG_DIR/input.prompt.md`, which could be read and asserted against.

**suggestion**: add assertions that verify prompt contents match criteria requirements (paths, BLOCKER/NITPICK instructions, template reference).

---

## verdict

- **3 blockers** must be addressed before merge (test coverage gaps for version selection, roadmap target, combined targets with missing files)
- **4 nitpicks** are suggestions for improvement

the core skill functionality is correctly implemented. the primary gaps are in test coverage to prove specific behaviors declared in criteria.
