wish =

we had a skill called `review.deliverable`


which took an input of --behavior

found the behavior with that name (if multiple, just failfast)


then asked for which declaration to review against
--against criteria | blueprint | roadmap | wish | vision


then grabbed the contents of what to review against

e.g., if criteria, get 2.criteria.md, failfast if dne
e.g., if blueprint, get 3.3.blueprint.vN.iM.md, failfast if dne
  - but get the LATEST version of N and M
  - e.g., if both 3.3.blueprint.v3.i2.md and 3.3.blueprint.v2.i3.md exist, pick v3.i2


then build a prompt to invoke claude code with

1. here is the behavior declaration to review against = ${path of file}

2. review the implementation against that behavior

3. emit a list of BLOCKERs and NITPICKs into 7.review.[feedback].vN.[given].by_robot.md

follow the feedback template in .ref.[feedback].v1.[given].by_human

and include at the top which skill was run to produce it



---

importantly, wish we also had a jest test suite declared which invoked that skill via subshell and proved that on some `.test/asset/example.repo/`s, it does the right thing

that way we can evolve this review skill over time reliably

collocate the test suite next to the skill

e.g.,

`src/domain.roles/behaver/skills/review.deliverable.integration.test.ts`

which leverages

`src/domain.roles/behaver/skills/.test/assets/example.repo/{x, y, z}/*`


-----

note, today the `brain` === `replic:claude-code`

in the future, it will be any brain, e.g., from `bhrain`

---

for context

- brains come in two fundamental frames
  - atomic = llm direct invocation
    - e.g., `atomic:openai:o4`
    - e.g., `atomic:anthropic:claude-opus-v4.3`
  - replic = llm behind a repl (read-execute-print-loop)
    - e.g., `replic:openai:codex`
    - e.g., `replic:anthropic:claude-code`
